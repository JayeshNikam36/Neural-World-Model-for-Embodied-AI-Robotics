{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_latent_dynamics.ipynb\n",
    "# =======================================\n",
    "# Action-Conditioned Latent Dynamics Training\n",
    "# Uses pretrained VAE → predicts next latent from current latent + action\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1. Imports & Configuration\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from models.data_loader import RoboNetDataset\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = r\"E:\\NVIDIA_PROJECTS\\Neural-World-Model-for-Embodied-AI-Robotics\"\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"robonet\", \"hdf5\")\n",
    "SPLITS_PATH = os.path.join(PROJECT_ROOT, \"data\", \"splits.json\")\n",
    "VAE_CHECKPOINT = os.path.join(PROJECT_ROOT, \"checkpoints\", \"vae_pretrained.pth\")\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "LATENT_DIM = 128  # must match your pretrained VAE\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2. Load splits\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "with open(SPLITS_PATH, \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "train_files = splits[\"train\"]\n",
    "val_files   = splits[\"val\"]\n",
    "\n",
    "print(f\"Train: {len(train_files)} | Val: {len(val_files)}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3. Datasets & Loaders\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "train_dataset = RoboNetDataset(train_files, DATA_ROOT)\n",
    "val_dataset   = RoboNetDataset(val_files,   DATA_ROOT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 4. Load Pretrained VAE & Freeze\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    # Copy your VAE class definition here (encoder, reparameterize, decoder, forward)\n",
    "    # ... paste the exact VAE class you used in pretraining ...\n",
    "\n",
    "vae = VAE(latent_dim=LATENT_DIM).to(DEVICE)\n",
    "vae.load_state_dict(torch.load(VAE_CHECKPOINT))\n",
    "vae.eval()\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  # freeze VAE\n",
    "\n",
    "print(\"Pretrained VAE loaded and frozen\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 5. Latent Dynamics Model (RSSM-style)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class LatentDynamics(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple recurrent latent predictor:\n",
    "    h_t = GRU(h_{t-1}, z_{t-1}, a_t)\n",
    "    z_t ~ N(mu, std) from h_t\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(latent_dim * 2 + 4, hidden_dim)  # z_prev + a + h_prev\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, z_prev, action, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(z_prev.size(0), self.gru.hidden_size, device=z_prev.device)\n",
    "\n",
    "        inp = torch.cat([z_prev, action, h_prev], dim=-1)\n",
    "        h = self.gru(inp, h_prev)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z_next = self.reparameterize(mu, logvar)\n",
    "        return z_next, mu, logvar, h\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "dynamics = LatentDynamics(LATENT_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(dynamics.parameters(), lr=LR)\n",
    "\n",
    "def dynamics_loss(z_pred, z_true, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(z_pred, z_true, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + 0.1 * kl_loss  # beta=0.1 for KL\n",
    "\n",
    "print(\"Latent dynamics model created. Parameters:\", sum(p.numel() for p in dynamics.parameters()))\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 6. Training Loop\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "checkpoint_path = os.path.join(CHECKPOINT_DIR, \"latent_dynamics_best.pth\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    dynamics.train()\n",
    "    train_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        frames = batch['frames'].to(DEVICE)   # (B, seq_len, C, H, W)\n",
    "        actions = batch['actions'].to(DEVICE) # (B, seq_len-1, 4)\n",
    "\n",
    "        B, T = frames.shape[0], frames.shape[1]\n",
    "\n",
    "        # Get latents for all frames\n",
    "        with torch.no_grad():\n",
    "            x_flat = frames.view(-1, 3, 128, 128)\n",
    "            z_all, _, _ = vae.encode(x_flat)  # (B*T, latent_dim)\n",
    "            z_all = z_all.view(B, T, LATENT_DIM)\n",
    "\n",
    "        loss = 0.0\n",
    "        h = None\n",
    "        for t in range(T-1):\n",
    "            z_curr = z_all[:, t]\n",
    "            a_t = actions[:, t]\n",
    "            z_next_pred, mu, logvar, h = dynamics(z_curr, a_t, h)\n",
    "\n",
    "            z_next_true = z_all[:, t+1]\n",
    "            loss += dynamics_loss(z_next_pred, z_next_true, mu, logvar)\n",
    "\n",
    "        loss = loss / (T-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / num_batches\n",
    "\n",
    "    # Validation (simplified)\n",
    "    dynamics.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            frames = batch['frames'].to(DEVICE)\n",
    "            actions = batch['actions'].to(DEVICE)\n",
    "            B, T = frames.shape[0], frames.shape[1]\n",
    "\n",
    "            x_flat = frames.view(-1, 3, 128, 128)\n",
    "            z_all = vae.encode(x_flat)[0].view(B, T, LATENT_DIM)\n",
    "\n",
    "            h = None\n",
    "            batch_loss = 0.0\n",
    "            for t in range(T-1):\n",
    "                z_curr = z_all[:, t]\n",
    "                a_t = actions[:, t]\n",
    "                z_next_pred, mu, logvar, h = dynamics(z_curr, a_t, h)\n",
    "                z_next_true = z_all[:, t+1]\n",
    "                batch_loss += dynamics_loss(z_next_pred, z_next_true, mu, logvar).item()\n",
    "\n",
    "            val_loss += batch_loss / (T-1)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(dynamics.state_dict(), checkpoint_path)\n",
    "        print(f\"Best dynamics model saved (val loss: {best_val_loss:.4f})\")\n",
    "\n",
    "print(\"Latent dynamics training finished!\")\n",
    "print(f\"Best checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 7. Quick Rollout Visualization (1-step prediction)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "dynamics.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(val_loader))\n",
    "    frames = batch['frames'].to(DEVICE)\n",
    "    actions = batch['actions'].to(DEVICE)\n",
    "\n",
    "    # Encode all frames\n",
    "    x_flat = frames.view(-1, 3, 128, 128)\n",
    "    z_all = vae.encode(x_flat)[0].view(frames.shape[0], -1, LATENT_DIM)\n",
    "\n",
    "    # Predict next latent from middle\n",
    "    t = frames.shape[1] // 2\n",
    "    z_curr = z_all[:, t]\n",
    "    a_t = actions[:, t]\n",
    "    z_next_pred, _, _, _ = dynamics(z_curr, a_t)\n",
    "\n",
    "    # Decode predicted latent\n",
    "    recon_pred = vae.decode(z_next_pred)\n",
    "\n",
    "    # Ground truth next frame\n",
    "    target = frames[:, t+1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(frames[0, t].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0].set_title(\"Current (t)\")\n",
    "    axes[1].imshow(recon_pred[0].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[1].set_title(\"Predicted (t+1)\")\n",
    "    axes[2].imshow(target[0].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[2].set_title(\"Actual (t+1)\")\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaf5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8657f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
